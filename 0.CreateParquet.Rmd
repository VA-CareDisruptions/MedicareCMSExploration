---
title: "index"
author: "Dan Weinberger"
date: '2022-12-28'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(arrow)
library(dplyr)
library(ggplot2)

source('./R/schema.R')
 
```

This Rmd file converts CSV files to parquet databases using the arrow package. 

denom files for 2014-2016; after 2016, shifts to MBSF files

```{r}
csv_file_denom <- "T:/denom/den_saf_lds_100_201.csv"

dest_denom <- "T:/parquet/denom/" 

#double()
#string()
#float()
#int32()
#https://www.cms.gov/Research-Statistics-Data-and-Systems/Files-for-Order/LimitedDataSets/Downloads/SAFldsDenomNov2009.pdf

csv_stream_denom <- open_dataset("T:/denom/", format = "csv", schema = sch_denom)
```

Write the data to a parquet database
```{r, eval=F}
write_dataset(csv_stream_denom, dest_denom, format = "parquet", 
              max_rows_per_file=1000000L,
              hive_style = TRUE,
              existing_data_behavior = "overwrite")
```

Do the same for MBSF data

Note on partioning: 
"Avoid files smaller than 20MB and larger than 2GB.
Avoid partitioning layouts with more than 10,000 distinct partitions."
https://ursalabs.org/arrow-r-nightly/articles/dataset.html

```{r, eval=F}
dest_mbsf <- "T:/parquet/mbsf/" 

csv_stream_mbsf <- open_dataset("T:/mbsf/", format = "csv", schema = sch_dbsf) %>%
  group_by(REFERENCE_YEAR) #ensure chunks are not too large for memory

write_dataset(csv_stream_mbsf, dest_mbsf, format = "parquet", 
               max_rows_per_file = 5e5,
              max_open_files=20, #default is 900L
              hive_style = TRUE,
              existing_data_behavior = "overwrite")
```

Inpatient base files

```{r}
dest_inp <- "T:/parquet/inp/" 

csv_stream_inp <- open_dataset("T:/inp/inp1/basek2016_2021", format = "csv", schema = sch_inp_basek)  #%>%
  #group_by(STATE_CD) #ensure chunks are not too large for memory

write_dataset(csv_stream_inp, dest_inp, format = "parquet", 
               max_rows_per_file = 1e5,
              #max_open_files=20, #default is 900L
              hive_style = TRUE,
              existing_data_behavior = "overwrite")
```

outpatient base files

```{r}
dest_outp <- "T:/parquet/outp/" 

csv_stream_outp <- open_dataset("T:/out/basek2016_2021", format = "csv", schema = sch_outp_basek)  %>%
  group_by(STATE_CD) #ensure chunks are not too large for memory

write_dataset(csv_stream_outp, dest_outp, format = "parquet", 
               max_rows_per_file = 5e5,
              #max_open_files=20, #default is 900L
              hive_style = TRUE,
              existing_data_behavior = "overwrite")
```



Skilled nursing

```{r}
dest_snf <- "T:/parquet/snf/" 

csv_stream_snf <- open_dataset("T:/snf/claimsk2016_2021/", format = "csv", schema = sch_snf_basek) 

write_dataset(csv_stream_snf, dest_snf, format = "parquet", 
               max_rows_per_file = 3e5,
              #max_open_files=20, #default is 900L
              hive_style = TRUE,
              existing_data_behavior = "overwrite")
```
