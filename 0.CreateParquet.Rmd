---
title: "index"
author: "Dan Weinberger"
date: '2022-12-28'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(arrow)
library(dplyr)
library(ggplot2)
library(parquetize)
source('./R/schema.R')
 
```

This Rmd file converts CSV files to parquet databases using the arrow package. 

denom files for 2014-2016; after 2016, shifts to MBSF files

```{r}
csv_file_denom <- "T:/denom/den_saf_lds_100_201.csv"

dest_denom <- "T:/parquet/denom/" 

#double()
#string()
#float()
#int32()
#https://www.cms.gov/Research-Statistics-Data-and-Systems/Files-for-Order/LimitedDataSets/Downloads/SAFldsDenomNov2009.pdf

csv_stream_denom <- open_dataset("T:/denom/", format = "csv", schema = sch_denom)
```

Write the data to a parquet database
```{r, eval=F}
write_dataset(csv_stream_denom, dest_denom, format = "parquet", 
              max_rows_per_file=1000000L,
              hive_style = TRUE,
              existing_data_behavior = "overwrite")
```

Do the same for MBSF data

Note on partitioning: 
"Avoid files smaller than 20MB and larger than 2GB.
Avoid partitioning layouts with more than 10,000 distinct partitions."
https://ursalabs.org/arrow-r-nightly/articles/dataset.html

```{r, eval=F}
dest_mbsf <- "T:/parquet/mbsf/" 

csv_stream_mbsf <- open_dataset("T:/mbsf/", format = "csv", schema = sch_dbsf) %>%
  group_by(REFERENCE_YEAR) #ensure chunks are not too large for memory

write_dataset(csv_stream_mbsf, dest_mbsf, format = "parquet", 
               max_rows_per_file = 5e5,
              #max_open_files=20, #default is 900L
              hive_style = TRUE,
              existing_data_behavior = "overwrite")
```

Inpatient base files

```{r}

csv_stream_inp_k <- open_dataset("T:/inp/inp1/basek2016_2021", format = "csv", schema = sch_inp_basek) #%>%
 # dplyr::select(DSYSRTKY,CLAIMNO,THRU_DT, starts_with('ICD'),starts_with('PRCDR'), TOT_CHRG,ADMSN_DT,TYPE_ADM,SRC_ADMS,PCCHGAMT,
             #  GNDR_CD,PTNTSTUS, DOB_DT, RACE_CD,CNTY_CD,STATE_CD,DSCHRGDT, UTIL_DAY, COIN_DAY,PRNCPAL_DGNS_CD)

csv_stream_inp_j <- open_dataset("T:/inp/inp1/basej_2014_2015", format = "csv", schema = sch_inp_basej)  %>%
   rename_with(toupper)

#common_names = intersect(names(csv_stream_inp_k), names(csv_stream_inp_j))

#csv_stream_inp_k <- csv_stream_inp_k%>%
#  dplyr::select(all_of(common_names))

#csv_stream_inp_j <- csv_stream_inp_j%>%
#  dplyr::select(all_of(common_names))

write_dataset(csv_stream_inp_j, "T:/parquet/inp_j/", format = "parquet", 
               max_rows_per_file = 1e5,
              #max_open_files=20, #default is 900L
              hive_style = TRUE,
              existing_data_behavior = "overwrite")

write_dataset(csv_stream_inp_k, "T:/parquet/inp_k_alt/", format = "parquet", 
               max_rows_per_file = 5e6,
           #   max_open_files=20, #default is 900L
              hive_style = TRUE,
              existing_data_behavior = "overwrite")


```


Inpatient charge files

```{r}
csv_stream_inp_k_chg <- open_dataset("T:/inp/inp2/", format = "csv", schema = sch_inp_rev) 

write_dataset(csv_stream_inp_k_chg, "T:/parquet/inp_k_chg/", format = "parquet", 
               max_rows_per_file = 5e6,
           #   max_open_files=20, #default is 900L
              hive_style = TRUE,
              existing_data_behavior = "overwrite")

```




outpatient base files
```{r, eval=F}
csv_stream_outp_k <- open_dataset("T:/out/basek2016_2021/", format = "csv", schema = sch_outp_basek)

# write_dataset(csv_stream_outp_k, "T:/parquet/outp_k/", format = "parquet", 
#                max_rows_per_file = 5e6,
#               hive_style = TRUE,
#               existing_data_behavior = "overwrite")

```

outpatient charge files

```{r}

csv_stream_outp_charge <- open_dataset("T:/out/charge_k", format = "csv", schema = sch_outp_rev)%>%
  dplyr::select(DSYSRTKY,CLAIMNO,THRU_DT,CLM_TYPE,REV_CNTR)

write_dataset(csv_stream_outp_charge, "T:/parquet/outp_k_charge/" , format = "parquet", 
               max_rows_per_file = 5e6,
              hive_style = TRUE,
              existing_data_behavior = "overwrite")
```

Skilled nursing

```{r}
dest_snf <- "T:/parquet/snf/" 

csv_stream_snf <- open_dataset("T:/snf/claimsk2016_2021/", format = "csv", schema = sch_snf_basek) 

write_dataset(csv_stream_snf, dest_snf, format = "parquet", 
               max_rows_per_file = 3e5,
              #max_open_files=20, #default is 900L
              hive_style = TRUE,
              existing_data_behavior = "overwrite")
```
